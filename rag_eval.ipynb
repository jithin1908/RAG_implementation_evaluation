{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6614acf3-1b75-4d86-b270-3c8a1bf35b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from rouge_score import rouge_scorer\n",
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce42d8a5-ea19-40fb-8623-421482d27e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your Excel file that contains question and answer along with llm generated one\n",
    "data = pd.read_excel(r\"C:\\Users\\jithi\\Downloads\\RAG_evaluation.csv\")\n",
    "# Assuming the DataFrame has columns 'Question' and 'True Answer'\n",
    "questions = data['Question'].tolist()\n",
    "true_answers = data['Answer'].tolist()\n",
    "generated_answers=data['LLM_answer'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ea5e8-c74c-4b41-ae0f-a913e2fef9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate ROUGE scores\n",
    "def calculate_rouge(generated, reference):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, generated)\n",
    "    return {key: value.fmeasure for key, value in scores.items()}\n",
    "\n",
    "# Function to calculate BLEU score\n",
    "def calculate_bleu(generated, reference):\n",
    "    reference_tokens = word_tokenize(reference.lower())\n",
    "    generated_tokens = word_tokenize(generated.lower())\n",
    "    bleu_score = sentence_bleu([reference_tokens], generated_tokens)\n",
    "    return bleu_score\n",
    "\n",
    "# Function to calculate perplexity\n",
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    inputs = tokenizer.encode(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs, labels=inputs)\n",
    "    loss = outputs.loss\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    return perplexity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a20b00-5116-4650-8f76-d17c99fedb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    
    "\n",
    "# Prepare lists to store scores\n",
    "rouge_scores_list = []\n",
    "bleu_scores_list = []\n",
    "perplexity_list = []\n",
    "\n",
    "# Calculate and store metrics for each question\n",
    "for generated, true in zip(generated_answers, true_answers):\n",
    "    rouge_scores = calculate_rouge(generated, true)\n",
    "    bleu_score = calculate_bleu(generated, true)\n",
    "    perplexity = calculate_perplexity(generated, model, tokenizer)\n",
    "    \n",
    "    # Store scores\n",
    "    rouge_scores_list.append(rouge_scores['rouge1'])  # You can change to 'rouge2' or 'rougeL' as needed\n",
    "    bleu_scores_list.append(bleu_score)\n",
    "    perplexity_list.append(perplexity)\n",
    "\n",
    "# Add scores to the DataFrame\n",
    "data['ROUGE Score'] = rouge_scores_list\n",
    "data['BLEU Score'] = bleu_scores_list\n",
    "data['Perplexity'] = perplexity_list\n",
    "data.to_csv(\"rag_eval.csv\")#store the output to the excel file\n",
    "# sample evaluated file is given"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
